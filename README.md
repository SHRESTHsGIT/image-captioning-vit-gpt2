# 🖼️ image-captioning-vit-gpt2

## ✨ vision-to-text

**vision-to-text** is an image captioning project that combines the visual understanding capabilities of **Vision Transformers (ViT)** with the language generation power of **GPT-2**.  
This model is trained on the **Flickr8k dataset**, which contains 8,000 images, each paired with five human-annotated captions. It generates fluent and contextually accurate descriptions of input images.

---

## 🚀 Key Features

- **ViT Encoder**: Extracts rich visual embeddings from input images.
- **GPT-2 Decoder**: Translates visual features into fluent natural language captions.
- **Flickr8k Dataset**: Compact, well-annotated dataset ideal for experimentation and benchmarking.
- **End-to-End Pipeline**: Fully integrated flow for preprocessing, encoding, decoding, and evaluation.
- **Evaluation Tools**: Supports standard NLP metrics such as **BLEU**, with options to integrate **ROUGE** and **CIDEr**.

---

## 📦 Tech Stack

- 🐍 Python & PyTorch  
- 🤗 Hugging Face Transformers  
- 🔍 Vision Transformer (ViT)  
- 🧠 GPT-2 Language Model  
- 🖼️ Flickr8k Dataset  

---

## 📁 Use Cases

- ♿ Describing visual content for the visually impaired
- 📸 Auto-captioning for social media, photography platforms
- 🔍 Improving image search and indexing systems
- 🤖 Foundation for multimodal AI applications (e.g., VQA, storytelling)

---

Feel free to contribute, fork, or explore improvements.  
Let's bridge vision and language! 🌉🧠
