# ğŸ–¼ï¸ image-captioning-vit-gpt2

## âœ¨ vision-to-text

**vision-to-text** is an image captioning project that combines the visual understanding capabilities of **Vision Transformers (ViT)** with the language generation power of **GPT-2**.  
This model is trained on the **Flickr8k dataset**, which contains 8,000 images, each paired with five human-annotated captions. It generates fluent and contextually accurate descriptions of input images.

---

## ğŸš€ Key Features

- **ViT Encoder**: Extracts rich visual embeddings from input images.
- **GPT-2 Decoder**: Translates visual features into fluent natural language captions.
- **Flickr8k Dataset**: Compact, well-annotated dataset ideal for experimentation and benchmarking.
- **End-to-End Pipeline**: Fully integrated flow for preprocessing, encoding, decoding, and evaluation.
- **Evaluation Tools**: Supports standard NLP metrics such as **BLEU**, with options to integrate **ROUGE** and **CIDEr**.

---

## ğŸ“¦ Tech Stack

- ğŸ Python & PyTorch  
- ğŸ¤— Hugging Face Transformers  
- ğŸ” Vision Transformer (ViT)  
- ğŸ§  GPT-2 Language Model  
- ğŸ–¼ï¸ Flickr8k Dataset  

---

## ğŸ“ Use Cases

- â™¿ Describing visual content for the visually impaired
- ğŸ“¸ Auto-captioning for social media, photography platforms
- ğŸ” Improving image search and indexing systems
- ğŸ¤– Foundation for multimodal AI applications (e.g., VQA, storytelling)

---

Feel free to contribute, fork, or explore improvements.  
Let's bridge vision and language! ğŸŒ‰ğŸ§ 
