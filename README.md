# image-captioning-vit-gpt2
##vision-to-text
vision-to-text is an image captioning project that combines the visual understanding power of Vision Transformers (ViT) with the language generation capabilities of GPT-2. This model generates accurate and fluent captions for images using the Flickr8k dataset, which consists of 8,000 images each paired with five human-written captions.

#🚀 Key Features
ViT Encoder: Extracts rich visual embeddings from input images.

GPT-2 Decoder: Converts visual features into descriptive natural language captions.

Flickr8k Dataset: Trained and evaluated on a well-annotated, compact dataset ideal for rapid experimentation.

End-to-End Pipeline: Seamlessly integrates image preprocessing, feature extraction, and text generation.

Evaluation Tools: Includes standard metrics BLEU.

#📦 Tech Stack
Python & PyTorch

Hugging Face Transformers

Vision Transformer (ViT)

GPT-2 Language Model

Flickr8k Dataset

#📁 Use Cases
Assisting visually impaired users through image descriptions

Automated captioning for social media or content platforms

Enhancing image search and indexing

Building blocks for multimodal AI applications

